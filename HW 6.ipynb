{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML HW6\n",
    "### 106598018 è¬ä¿Šç‘‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. We mentioned that the parameters ğ›¾ and ğ›½ in batch normalization are trained through backprop. Explain why we donâ€™t want to directly compute these two parameters from the training samples and also give the details how the training is carried out.**  \n",
    ">Beacause the mean and varence of data in each batch are different, so we need to train the ğ›¾ and ğ›½ like training the weight, but not compute they from the all data set.  \n",
    "And we can train the ğ›¾ and ğ›½ in the same way which we train the weight in NN. We can compute the partial derivative of loss function for ğ›¾ and ğ›½ and do the gradient decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. We use the â€œReparameterization Trickâ€ in training Variational AutoEncoder (VAE). Explain why this trick is necessary and how it is accomplished.**\n",
    ">The reparameterization trick is as follows. Recall, if we have xâˆ¼N(Î¼,Î£) and then standardize it so that Î¼=0,Î£=1, we could revert it back to the original distribution by reverting the standardization process. Hence, we have this equation:x=Î¼+Î£1/2Xstd  \n",
    "With that in mind, we could extend it. If we sample from a standard normal distribution, we could convert it to any Gaussian we want if we know the mean and the variance. Hence we could implement our sampling operation of \n",
    "z by:z=Î¼(X)+Î£1/2(X)Ïµ where Ïµâˆ¼N(0,1).  \n",
    "Now, during backpropagation, we donâ€™t care anymore with the sampling process, as it is now outside of the network, i.e. doesnâ€™t depend on anything in the net, hence the gradient wonâ€™t flow through it.  \n",
    "However, we are now facing a problem. How do we get z from the encoder outputs? Obviously we could sample z from a Gaussian which parameters are the outputs of the encoder. Alas, sampling directly wonâ€™t do, if we want to train VAE with gradient descent as the sampling operation doesnâ€™t have gradient!  \n",
    "There is, however a trick called reparameterization trick, which makes the network differentiable. Reparameterization trick basically divert the non-differentiable operation out of the network, so that, even though we still involve a thing that is non-differentiable, at least it is out of the network, hence the network could still be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Use the equations of optimal margin (linear) SVM (in pp. 12) to find ğ° given ğ±1 =[1 âˆ’1]ğ‘‡ âˆˆğ¶+1 and ğ±2 =[âˆ’1 âˆ’1]ğ‘‡ âˆˆğ¶âˆ’1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Implement a discrete HMM training program. Use the three-urn example (in pp.\n",
    "12 of the PPT file) to test your program and produce the training results after 100\n",
    "iterations. Use the red and blue balls in each urn to compute the initial emission\n",
    "111 333\n",
    "probability. The initial transition probability A = 1 1 1 and Ï€ = [1 1 1]. 333 333\n",
    "111 [3 3 3]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
